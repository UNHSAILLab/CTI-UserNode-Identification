{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import bson\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.summarization import keywords\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import io\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "import networkx as nx\n",
    "import math\n",
    "from tqdm import trange, tqdm\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from networkx.algorithms import community\n",
    "import community as community_louvain\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import igraph as ig\n",
    "import louvain\n",
    "import leidenalg\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "import hdbscan\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"how're\": \"how are\",\n",
    "\"how's\": \"how is\",\n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token in contractions:\n",
    "            tokens[idx]=contractions[token]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_compound(input_termList):\n",
    "#     if not input_termList or input_termList == None or input_termList == \"\":\n",
    "#         return input_termList\n",
    "    # maximum edit distance per dictionary precalculation\n",
    "    symSpell_1st_TotalTweetText=[]\n",
    "    max_edit_distance_dictionary = 2\n",
    "    prefix_length = 7\n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "    # load dictionary\n",
    "    dictionary_path = os.path.join(os.path.dirname(\"E:\\\\Threat_Intel_Research\\\\\"),\"frequency_dictionary_en_82_765.txt\")\n",
    "    term_index = 0  # column of the term in the dictionary text file\n",
    "    count_index = 1  # column of the term frequency in the dictionary text file\n",
    "    if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "        print(\"Dictionary file not found\")\n",
    "        return\n",
    "\n",
    "    # max edit distance per lookup (per single word, not per whole input string)\n",
    "    max_edit_distance_lookup = 2\n",
    "    input_term2 = re.sub(r\"http\\S+\", \"\", input_termList)\n",
    "    suggestions = sym_spell.lookup_compound(input_term2, max_edit_distance_lookup)\n",
    "    for suggestion in suggestions:\n",
    "        symSpell_1st_TotalTweetText.append(suggestion.term)\n",
    " \n",
    "    return symSpell_1st_TotalTweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLookUp(input_termLookList):\n",
    "#     if not input_termLookList or input_termLookList == None or input_termLookList == \"\":\n",
    "#         return input_termLookList\n",
    "    symSpell_2nd_TotalTweetText=[]\n",
    "    # maximum edit distance per dictionary precalculation\n",
    "    max_edit_distance_dictionary = 0\n",
    "    prefix_length = 7\n",
    "    # create object\n",
    "    sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "    # load dictionary\n",
    "    dictionary_path = os.path.join(os.path.dirname(\"E:\\\\Threat_Intel_Research\\\\\"),\"frequency_dictionary_en_82_765.txt\")\n",
    "    term_index = 0  # column of the term in the dictionary text file\n",
    "    count_index = 1  # column of the term frequency in the dictionary text file\n",
    "    if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "        print(\"Dictionary file not found\")\n",
    "        return\n",
    "\n",
    "    # a sentence without any spaces\n",
    "    if input_termLookList:\n",
    "        result = sym_spell.word_segmentation(input_termLookList)\n",
    "        symSpell_2nd_TotalTweetText.append(result.corrected_string)\n",
    "    return symSpell_2nd_TotalTweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    #doc = re.sub(r\\\"http\\\\S+\\\", \\\"\\\", doc)\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords)\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    #tokens = [stemmer.stem(t) for t in tokens]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = extend_doc(text)\n",
    "    textSpell = hook_compound(text)\n",
    "    if len(textSpell[0])>=5:\n",
    "        textSpell = mainLookUp(textSpell[0])\n",
    "        cleanedText = clean_doc(textSpell[0])\n",
    "    else:\n",
    "        cleanedText=\"NAAAN\"\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "def apply_TFIDF(fname):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.99,lowercase=True, max_features=200000,min_df=0.001, stop_words='english', use_idf=True)\n",
    "    #check_mat=tfidf_vectorizer.fit(fname)\n",
    "    #print(\"check \", check_mat.vocabulary_)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(fname) #fit the vectorizer to synopses\n",
    "    #print(tfidf_matrix.shape, tfidf_matrix[0,0], tfidf_matrix[0,1], tfidf_matrix[0,2], tfidf_matrix[0,3])\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    #print(tfidf_matrix.shape, \" + \",terms, tfidf_vectorizer.idf_)\n",
    "    print(\" + \", tfidf_matrix.shape, \" + \")\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userObjectDictionaryFunction():\n",
    "    connection=pymongo.MongoClient()\n",
    "    srcUserDB=connection.user_src_identification\n",
    "    collection=srcUserDB.level1_tagged_2K_user\n",
    "    cursor=collection.find({},{\"_id\":0}).limit(1000)\n",
    "    userObjectData=[]\n",
    "    userIdDict=dict()\n",
    "    for data in cursor:\n",
    "        userDataDict=dict()\n",
    "        for key, value in data.items():\n",
    "            if key==\"id\":\n",
    "                userID=value\n",
    "            if key==\"Description\":\n",
    "                userDataDict.update({\"Description\":value})\n",
    "            if key==\"DescTag\":\n",
    "                userDataDict.update({\"DescTag\":value})\n",
    "            if key==\"Followers\":\n",
    "                userDataDict.update({\"Followers\":value})\n",
    "            if key==\"Following\":\n",
    "                userDataDict.update({\"Following\":value})\n",
    "            if key==\"Tweets\":\n",
    "                userDataDict.update({\"Tweets\":value})\n",
    "            if key==\"TweetsTags\":\n",
    "                userDataDict.update({\"TweetsTags\":value})\n",
    "            if key==\"AllText\":\n",
    "                userDataDict.update({\"AllText\":value})\n",
    "            if key==\"AllTextTags\":\n",
    "                userDataDict.update({\"AllTextTags\":value})\n",
    "        userIdDict.update({int(userID):userDataDict})\n",
    "    return userIdDict\n",
    "userIdDict = userObjectDictionaryFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Json_Processed_Data_File():\n",
    "    for userID, entries in userIdDict.items():\n",
    "        if \"Tweets\" in entries:\n",
    "            if len(entries[\"Tweets\"]) != 0:\n",
    "                processedTweetTextList = list()\n",
    "                for tweetText in entries[\"Tweets\"]:\n",
    "                    if len(tweetText) <= 5:\n",
    "                        continue\n",
    "                    else:\n",
    "                        processedTweetTextList.append(process_text(tweetText))\n",
    "                entries.update({\"Tweets\":processedTweetTextList})\n",
    "            else:\n",
    "                del entries[\"Tweets\"]\n",
    "        if \"AllText\" in entries:\n",
    "            if len(entries[\"AllText\"]) >= 5:\n",
    "                entries.update({\"AllText\":process_text(entries[\"AllText\"])})\n",
    "            else:\n",
    "                del entries[\"AllText\"]\n",
    "        if \"Description\" in entries:\n",
    "            if len(entries[\"Description\"]) >= 5:\n",
    "                entries.update({\"Description\":process_text(entries[\"Description\"])})\n",
    "            else:\n",
    "                del entries[\"Description\"]\n",
    "        if 'DescTag' in entries:\n",
    "            if len(entries['DescTag']) == 0:\n",
    "                del entries['DescTag']\n",
    "    with open('userIdDict.json', 'w') as fp:\n",
    "        json.dump(userIdDict, fp)\n",
    "create_Json_Processed_Data_File()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryAnalyzer(tweetID, Type):\n",
    "    userScoreDict=dict()\n",
    "    score=0\n",
    "    keepOrDiscard=True\n",
    "    if tweetID not in userIdDict:\n",
    "        userScoreDict[tweetID] = 0\n",
    "        keepOrDiscard = False\n",
    "        return keepOrDiscard, userScoreDict\n",
    "    if Type == \"TweetsTags\" or Type==True:\n",
    "        countTech=0\n",
    "        countSci=0\n",
    "        for per_TextID, categoryList in enumerate(userIdDict[tweetID][\"TweetsTags\"]):\n",
    "            for tagID, category in enumerate(categoryList):\n",
    "                if \"technology and computing\" in category:\n",
    "                    if (countTech%5)==0:\n",
    "                        score=score+0.5\n",
    "                    countTech=countTech+1\n",
    "                    break\n",
    "                if \"computer science\" in category:\n",
    "                    if (countSci%5)==0:\n",
    "                        score=score+0.1\n",
    "                    countSci=countSci+1\n",
    "        if countTech<=2:\n",
    "            userScoreDict[tweetID] = 0\n",
    "            keepOrDiscard = False\n",
    "    if Type == \"DescTag\" or Type==True:\n",
    "        countTech=0\n",
    "        countSci=0\n",
    "        for tagID, category in enumerate(userIdDict[tweetID][\"DescTag\"]):\n",
    "            if \"technology and computing\" in category:\n",
    "                score=score+2\n",
    "                break\n",
    "            if \"computer science\" in category:\n",
    "                score=score+0.5\n",
    "    if keepOrDiscard==True:\n",
    "        userScoreDict[tweetID] = score\n",
    "#     else:\n",
    "#         print(\"Assign 0\")\n",
    "    return keepOrDiscard, userScoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoryAnalyzer(1201265355336798208, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followerNode=list()\n",
    "followingNode=list()\n",
    "for key, item in userIdDict.items():\n",
    "    for follrKey, followerVal in enumerate(item['Followers']):\n",
    "        followerNode.append((int(key), int(followerVal), 1.00))\n",
    "    for follngKey, followingVal in enumerate(item['Following']):\n",
    "        followingNode.append((int(followingVal), int(key), 1.00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followerDF = pd.DataFrame(followerNode, columns =['Src', 'Des', 'Score'])\n",
    "followingDF= pd.DataFrame(followingNode, columns = ['Src', 'Des', 'Score'])\n",
    "followRelnDF = pd.concat([followerDF,followingDF])\n",
    "followerDF.to_csv(\"followerDF.csv\", sep=',', index=False, header=False)\n",
    "followingDF.to_csv(\"followingDF.csv\", sep=',', index=False, header=False)\n",
    "followRelnDF.to_csv(\"followRelnDF.csv\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_reader():\n",
    "    \"\"\"\n",
    "    Function to read the graph from the path.\n",
    "    :param path: Path to the edge list.\n",
    "    :return graph: NetworkX object returned.\n",
    "    \"\"\"\n",
    "    graph=nx.read_edgelist('followRelnDF.csv', delimiter=',', data=(('weight',float),), nodetype=int, create_using=nx.DiGraph())\n",
    "    graph1=nx.read_edgelist('followerDF.csv', delimiter=',', data=(('weight',float),), nodetype=int, create_using=nx.DiGraph())\n",
    "    graph2=nx.read_edgelist('followingDF.csv', delimiter=',', data=(('weight',float),), nodetype=int, create_using=nx.DiGraph())\n",
    "    return graph, graph1, graph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph, graph1, graph2 = graph_reader()\n",
    "nx_graph.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nx.info(nx_graph))\n",
    "# nx.draw(nx_graph)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection using Girvan-Newman Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_generator = community.girvan_newman(nx_graph)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)\n",
    "sorted(map(sorted, next_level_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testList = sorted(map(sorted, next_level_communities))\n",
    "len(testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girvan_PartitionDict=dict()\n",
    "for idx, subGrList in enumerate(testList):\n",
    "    for ind, node in enumerate(subGrList):\n",
    "        girvan_PartitionDict.update({node:idx})\n",
    "# pos = nx.spring_layout(nx_graph)\n",
    "# cmap = cm.get_cmap('viridis', max(girvan_PartitionDict.values()) + 1)\n",
    "# nx.draw_networkx_nodes(nx_graph, pos, girvan_PartitionDict.keys(), node_size=40, cmap=cmap, node_color=list(girvan_PartitionDict.values()))\n",
    "# nx.draw_networkx_edges(nx_graph, pos, alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection on Leiden Algorithm (Networkx+igraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ig.Graph.Erdos_Renyi(100, 0.1, directed=True))\n",
    "# list(zip(*list(zip(*nx.to_edgelist(nx_graph)))[:2]))\n",
    "# type(nx.to_scipy_sparse_matrix(nx_graph))\n",
    "# nx_graph = nx.Graph()\n",
    "# len(nx_graph.nodes())\n",
    "# list1 = [(1, 177), (1, 54), (1, 61), (1, 86), (10, 100)]\n",
    "# nx_graph.add_edges_from(list1)\n",
    "# g = ig.Graph(nx_graph.edges())\n",
    "# g = ig.Graph([(99999999,1), (99999999,2)])\n",
    "# g = ig.Graph(len(nx_graph), list(nx_graph.edges()))\n",
    "# leiden_partition = leidenalg.find_partition(ig_graph, leidenalg.ModularityVertexPartition)\n",
    "# g = ig.Graph.Adjacency(nx.to_scipy_sparse_matrix(nx_graph))\n",
    "# print(g.ecount(), g.vs.indices, g.get_edgelist(), nx_graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxNode_to_Index_Mapping = {node:ind for ind, node in enumerate(list(nx_graph.nodes()))}\n",
    "index_to_nxNode_Mapping = {ind:node for ind, node in enumerate(list(nx_graph.nodes()))}\n",
    "nxNodeList = list(index_to_nxNode_Mapping.keys())\n",
    "mapped_Edgelist = [(nxNode_to_Index_Mapping[nodeTouple[0]], nxNode_to_Index_Mapping[nodeTouple[1]]) for nodeTouple in nx_graph.edges()]\n",
    "ig_graph = ig.Graph(len(nx_graph), mapped_Edgelist)\n",
    "# ig_graph = ig.Graph.Adjacency((nx.to_scipy_sparse_matrix(nx_graph) > 0).tolist())\n",
    "louvain_partition = louvain.find_partition(ig_graph, louvain.ModularityVertexPartition)\n",
    "leiden_partition = leidenalg.find_partition(ig_graph, leidenalg.ModularityVertexPartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeList=ig_graph.get_edgelist()\n",
    "nx_conv_graph = nx.DiGraph(edgeList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden_partitionDict= dict()\n",
    "for ind, nodeList in enumerate(list(leiden_partition)):\n",
    "    for node in nodeList:\n",
    "        leiden_partitionDict.update({node:ind})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = nx.spring_layout(nx_conv_graph)\n",
    "# cmap = cm.get_cmap('viridis', max(leiden_partitionDict.values()) + 1)\n",
    "# nx.draw_networkx_nodes(nx_conv_graph, pos, leiden_partitionDict.keys(), node_size=10, cmap=cmap, node_color=list(leiden_partitionDict.values()))\n",
    "# nx.draw_networkx_edges(nx_conv_graph, pos, alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leiden_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undirected Grpah\n",
    "print(louvain_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_user_membership_function():\n",
    "    communityScoreDict=dict()\n",
    "    allUserScoreDict = dict()\n",
    "    communityTextDict=dict()\n",
    "    igraph_nx_mapper = dict()\n",
    "    valid_UserNodes_in_Community = dict()\n",
    "    userNode_Community_map = dict()\n",
    "    for commID, community in enumerate(leiden_partition):\n",
    "#         nxNodeList = list(nx_graph.nodes())\n",
    "        communityTotalScore=0\n",
    "        communityText = \"\"\n",
    "        CTI_userList=list()\n",
    "        for index, memberNode in enumerate(community):\n",
    "            igraph_nx_mapper.update({memberNode:index_to_nxNode_Mapping[nxNodeList[memberNode]]})\n",
    "            \n",
    "            status, id_with_Score = categoryAnalyzer(index_to_nxNode_Mapping[nxNodeList[memberNode]], True)\n",
    "            \n",
    "#             status, id_with_Score = categoryAnalyzer(nxNodeList[memberNode], True)\n",
    "            \n",
    "            communityTotalScore = communityTotalScore + list(id_with_Score.values())[0]\n",
    "            allUserScoreDict.update(id_with_Score)\n",
    "            if status == True:\n",
    "                if userIdDict[index_to_nxNode_Mapping[nxNodeList[memberNode]]]['Tweets'] and userIdDict[index_to_nxNode_Mapping[nxNodeList[memberNode]]]['Tweets'] != None:\n",
    "                    Tweets=' '.join(userIdDict[index_to_nxNode_Mapping[nxNodeList[memberNode]]]['Tweets'])\n",
    "                    communityText = communityText + \" \" + Tweets\n",
    "                    CTI_userList.append(index_to_nxNode_Mapping[nxNodeList[memberNode]])\n",
    "                    userNode_Community_map.update({index_to_nxNode_Mapping[nxNodeList[memberNode]]:commID})\n",
    "        communityScoreDict[commID] = communityTotalScore/(len(community))\n",
    "        valid_UserNodes_in_Community[commID] = CTI_userList\n",
    "    #     print(\"communityScoreDict-> \", communityScoreDict[commID], communityText)\n",
    "        if communityText or communityText != \"\":\n",
    "            communityTextDict.update({commID:communityText})\n",
    "    return communityScoreDict, allUserScoreDict, communityTextDict, igraph_nx_mapper, valid_UserNodes_in_Community, userNode_Community_map\n",
    "communityScoreDict, allUserScoreDict, communityTextDict, igraph_nx_mapper, valid_UserNodes_in_Community, userNode_Community_map = community_user_membership_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communityTextDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_to_Index_Mapper = {node: i for i, node in enumerate(list(communityTextDict.keys()))}\n",
    "index_to_Community_Mapper = {i: node for i, node in enumerate(list(community_to_Index_Mapper.keys()))}\n",
    "# igraph_nx_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seedInput(seedList):\n",
    "    seedNode_CleanedTextList =  list()\n",
    "    for idx, nodeID in enumerate(seedList):\n",
    "        userObject = userIdDict[seedList[idx]]\n",
    "        if userObject['AllText'] and userObject['AllText'] != None and len(userObject['AllText'])>= 0:\n",
    "            seedNode_CleanedTextList.append(userObject['AllText'])\n",
    "    return seedNode_CleanedTextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed node input\n",
    "def text_tfIdf_Vectorization(commOrUser_TextDict_or_TextList, seed_TextList):\n",
    "    if isinstance(commOrUser_TextDict_or_TextList, dict):\n",
    "        sparse_tfIdfVector = apply_TFIDF(list(commOrUser_TextDict_or_TextList.values()) + seed_TextList)\n",
    "    if isinstance(commOrUser_TextDict_or_TextList, list):\n",
    "        sparse_tfIdfVector = apply_TFIDF(commOrUser_TextDict_or_TextList + seed_TextList)\n",
    "    return sparse_tfIdfVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedList = [1296940906302537730, 1089055000427298816]\n",
    "seedNode_CleanedTextList = seedInput(seedList)\n",
    "community_seed_sparse_tfIdfVector = text_tfIdf_Vectorization(communityTextDict, seedNode_CleanedTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_seed_sparse_tfIdfVector = community_seed_sparse_tfIdfVector.toarray()\n",
    "similarityMatrix = cosine_similarity(community_seed_sparse_tfIdfVector[-len(seedList):],community_seed_sparse_tfIdfVector[0:(community_seed_sparse_tfIdfVector.shape[0]-len(seedList))])\n",
    "# similarityMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = str(input(\"Seperate or All\"))\n",
    "MaxSimilartyList = [list(similarityMatrix[i].argsort()[-3:][::-1]) for i in range(similarityMatrix.shape[0])]\n",
    "print(MaxSimilartyList)\n",
    "communitySimScore = np.array([np.take(similarityMatrix[i], indices, axis=0) for i, indices in enumerate(MaxSimilartyList)])\n",
    "communityID = set([index_to_Community_Mapper[y] for x in MaxSimilartyList for y in x])\n",
    "\n",
    "MinSimilartyList = [list(similarityMatrix[i].argsort()[:-1]) for i in range(similarityMatrix.shape[0])]\n",
    "outlierCommunityID = set([index_to_Community_Mapper[y] for x in MinSimilartyList for y in x])\n",
    "outlierCommunityID = outlierCommunityID - communityID\n",
    "\n",
    "if method ==\"Sep\":\n",
    "    print(MaxSimilartyList,communitySimScore)\n",
    "    communityWeight = np.array([np.array([communityScoreDict[index_to_Community_Mapper[y]] for y in x]) for x in MaxSimilartyList])\n",
    "    value = np.add(communitySimScore*alpha, communityWeight)\n",
    "#     print(value, communityWeight)\n",
    "if method == \"All\":\n",
    "    #     MaxSimilartyArray = set(itertools.chain.from_iterable(MaxSimilartyList))\n",
    "    communitySimScoreFlat = communitySimScore.flatten('C')\n",
    "    print(MaxSimilartyList,communitySimScore, communitySimScoreFlat)\n",
    "    communityWeightFlat = np.array([communityScoreDict[index_to_Community_Mapper[y]] for x in MaxSimilartyList for y in x])\n",
    "    value = np.add(communitySimScoreFlat, communityWeightFlat)\n",
    "#     print(value, communityWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_Search_in_Community(valid_UserNodes, seed_TextList):\n",
    "    user_Score_in_CommunityDict = dict()\n",
    "    for commKey in communityID:\n",
    "        usrID_score_dict = dict()\n",
    "        if commKey in valid_UserNodes:\n",
    "            community_Member_TextList = [userIdDict[userID]['AllText'] for userID in valid_UserNodes[commKey]]\n",
    "            userNode_seed_sparse_tfIdfVector = text_tfIdf_Vectorization(community_Member_TextList, seed_TextList)\n",
    "            userNode_seed_sparse_tfIdfVector = userNode_seed_sparse_tfIdfVector.toarray()\n",
    "            print(\"userNode_seed_sparse_tfIdfVector-> \", userNode_seed_sparse_tfIdfVector.shape)\n",
    "            user_Seed_SimilarityMatrix = cosine_similarity(userNode_seed_sparse_tfIdfVector[-len(seedList):],userNode_seed_sparse_tfIdfVector[0:(userNode_seed_sparse_tfIdfVector.shape[0]-len(seedList))])\n",
    "            print(\"#########\", user_Seed_SimilarityMatrix)\n",
    "            if user_Seed_SimilarityMatrix.shape[1] <= 1:\n",
    "                user_Seed_MaxSimilartyArr = np.array([np.zeros(len(user_Seed_SimilarityMatrix.flatten()))])\n",
    "                userNode_SimScore = np.array(user_Seed_SimilarityMatrix).flatten()\n",
    "            else:\n",
    "                user_Seed_MaxSimilartyArr = np.array([user_Seed_SimilarityMatrix[i].argsort()[-3:][::-1] for i in range(user_Seed_SimilarityMatrix.shape[0])])\n",
    "                ### Problem\n",
    "                userNode_SimScore = np.array([np.take(user_Seed_SimilarityMatrix[i], list(user_Seed_MaxSimilartyArr[i,:]), axis=0) for i in range(user_Seed_MaxSimilartyArr.shape[0])])\n",
    "            user_Seed_MaxSimilartyArrFlat = user_Seed_MaxSimilartyArr.flatten('C')\n",
    "            userNode_SimScoreFlat = userNode_SimScore.flatten('C')\n",
    "            userNodeID = [valid_UserNodes[commKey][int(x)] for x in np.nditer(user_Seed_MaxSimilartyArrFlat)]\n",
    "            userNode_Weight = np.array([allUserScoreDict[y] for y in userNodeID])\n",
    "            userNode_Score_Array = np.add(userNode_SimScoreFlat*alpha, userNode_Weight)\n",
    "            print(\"user_Seed_MaxSimilartyArr-> \", user_Seed_MaxSimilartyArr,\" userNode_SimScoreFlat-> \",userNode_SimScoreFlat,\" userNode_Weight-> \", userNode_Weight)\n",
    "            for idx, nodeID in enumerate(userNodeID):\n",
    "                if nodeID in usrID_score_dict:\n",
    "                    if usrID_score_dict[nodeID]> userNode_Score_Array[idx]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        usrID_score_dict[nodeID] = userNode_Score_Array[idx]\n",
    "                else:\n",
    "                    usrID_score_dict[nodeID] = userNode_Score_Array[idx]\n",
    "            user_Score_in_CommunityDict[commKey] = usrID_score_dict\n",
    "    return user_Score_in_CommunityDict\n",
    "recommended_UserIDwith_Score = user_Search_in_Community(valid_UserNodes_in_Community, seedNode_CleanedTextList)\n",
    "print(recommended_UserIDwith_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_UserIDwith_Score = sorted(list(recommended_UserIDwith_Score.values()), key = lambda i: list(i.values())[0], reverse=True)[:10]\n",
    "outlierCommunityID = sorted(outlierCommunityID, key = lambda i: communityScoreDict[i])\n",
    "# [userID for commKey in communityID for userID in valid_UserNodes[commKey] if commKey in valid_UserNodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two lines Can be added\n",
    "highRank_Comm_User_map_list = [{(userNode_Community_map[list(id_value_map.keys())[0]]):(list(id_value_map.keys())[0])} for id_value_map in recommended_UserIDwith_Score]\n",
    "community_flag = set()\n",
    "for commID_usr_map in highRank_Comm_User_map_list:\n",
    "    if list(commID_usr_map.keys())[0] in community_flag:\n",
    "        continue\n",
    "    else:\n",
    "        community_flag.add(list(commID_usr_map.keys())[0])\n",
    "        \n",
    "        for usrID in valid_UserNodes_in_Community[list(commID_usr_map.keys())[0]]:\n",
    "            if usrID == list(commID_usr_map.values())[0]:\n",
    "                continue\n",
    "            if usrID in recommended_UserIDwith_Score and usrID != list(commID_usr_map.values())[0]:\n",
    "                recommended_UserIDwith_Score[usrID] = recommended_UserIDwith_Score[usrID] + recommended_UserIDwith_Score[list(commID_usr_map.values())[0]]*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_Batch_Sentences = [\"Here is the sentence I want embeddings for. This is because the BERT tokenizer was created with a WordPiece model.\", \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "def bertEmbeddingGenerator(inputType, user_Batch_Sentences):\n",
    "    if inputType == \"Base\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        encoded_input = tokenizer(user_Batch_Sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         print(encoded_input)\n",
    "        model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True, ) # Whether the model returns all hidden-states.\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        model.eval()\n",
    "\n",
    "        hidden_states = list()\n",
    "        # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_input['input_ids'], encoded_input['token_type_ids'], encoded_input['attention_mask'])\n",
    "            # As we set `output_hidden_states = True`, the third item will be the hidden states from all layers\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2, 3)\n",
    "#         print(\"token_embeddings-> \", token_embeddings.shape)\n",
    "        \n",
    "        userTextsEmbeddingArr = torch.stack([torch.mean(token_embeddings[i][-4:, :,], dim=0).mean(dim=0) for i in range(len(user_Batch_Sentences))]).mean(dim=0)\n",
    "        \n",
    "#         print(\"userTextsEmbeddingArr-> \", userTextsEmbeddingArr.shape)\n",
    "        \n",
    "        return userTextsEmbeddingArr\n",
    "\n",
    "    # DistilBert does not have'token_type_ids' \n",
    "    if inputType == \"Distil\":\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        encoded_input = tokenizer(user_Batch_Sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         print(encoded_input)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        hidden_states = list()\n",
    "        # Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "#         print(\"token_embeddings-> \", token_embeddings.shape)\n",
    "        userTextsEmbeddingList = [torch.mean(token_embeddings[i], dim=0).mean(dim=0) for i in range(len(user_Batch_Sentences))]\n",
    "        \n",
    "        userTextsEmbeddingArr = torch.stack(userTextsEmbeddingList, dim=0).numpy()\n",
    "#         print(\"userTextsEmbeddingArr-> \", userTextsEmbeddingArr.shape)\n",
    "        \n",
    "        return userTextsEmbeddingArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputType = str(input(\" BERT Model Type \"))\n",
    "validator_embedding_list = list()\n",
    "for key, userID_Score_Pair in enumerate(recommended_UserIDwith_Score):\n",
    "    user_Batch_Sentences = [text for indx, text, in enumerate(userIdDict[list(userID_Score_Pair.keys())[0]][\"Tweets\"])]\n",
    "    validator_embedding_list.append(bertEmbeddingGenerator(inputType, user_Batch_Sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"recommended_UserIDwith_Score-> \", recommended_UserIDwith_Score, \" outlierCommunityID-> \", outlierCommunityID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Has to be done by tonight\n",
    "for out_commKey in outlierCommunityID:\n",
    "    for userID in valid_UserNodes_in_Community[out_commKey]:\n",
    "        if userID:\n",
    "            out_user_Batch_Sentences = [text for indx, text, in enumerate(userIdDict[userID][\"Tweets\"])]\n",
    "            validator_embedding_list.append(bertEmbeddingGenerator(inputType, out_user_Batch_Sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = np.array([[0.5, 0.2, 6], [-2, 4, 9]])\n",
    "data = np.vstack(validator_embedding_list)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, gen_min_span_tree=True)\n",
    "cluster_labels = clusterer.fit_predict(data)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def user_Search_in_Community(valid_UserNodes, seed_TextList):\n",
    "#     user_Score_in_CommunityDict = dict()\n",
    "#     for commKey, CTI_userList in valid_UserNodes.items():\n",
    "#         usrID_score_dict = dict()\n",
    "#         if commKey in communityID:\n",
    "#             community_Member_TextList = [userIdDict[userID]['AllText'] for userID in CTI_userList]\n",
    "#             userNode_seed_sparse_tfIdfVector = text_tfIdf_Vectorization(community_Member_TextList, seed_TextList)\n",
    "#             userNode_seed_sparse_tfIdfVector = userNode_seed_sparse_tfIdfVector.toarray()\n",
    "#             user_Seed_SimilarityMatrix = cosine_similarity(userNode_seed_sparse_tfIdfVector[-len(seedList):],userNode_seed_sparse_tfIdfVector[0:(userNode_seed_sparse_tfIdfVector.shape[0]-len(seedList))])\n",
    "#             print(\"#########\", user_Seed_SimilarityMatrix)\n",
    "#             if user_Seed_SimilarityMatrix.shape[1] <= 1:\n",
    "#                 user_Seed_MaxSimilartyList = list(np.zeros(len(user_Seed_SimilarityMatrix.flatten())))\n",
    "#                 userNode_SimScore = np.array(user_Seed_SimilarityMatrix).flatten()\n",
    "#             else:\n",
    "#                 user_Seed_MaxSimilartyList = list(np.argmax(user_Seed_SimilarityMatrix, axis=1))\n",
    "#                 userNode_SimScore = np.amax(user_Seed_MaxSimilartyList, axis=1)\n",
    "#             userNodeID = [CTI_userList[int(x)] for x in user_Seed_MaxSimilartyList]\n",
    "#             userNode_Weight = np.array([allUserScoreDict[y] for y in userNodeID])\n",
    "#             userNode_Score_Array = np.add(userNode_SimScore, userNode_Weight)\n",
    "#             for idx, nodeID in enumerate(userNodeID):\n",
    "#                 usrID_score_dict[nodeID] = userNode_Score_Array[idx]\n",
    "#             user_Score_in_CommunityDict[commKey] = usrID_score_dict\n",
    "#     return user_Score_in_CommunityDict\n",
    "# print(user_Search_in_Community(valid_UserNodes_in_Community, seedNode_CleanedTextList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxSimilartyList = list(np.argmax(similarityMatrix, axis=1))\n",
    "# communitySimScore = np.amax(similarityMatrix, axis=1)\n",
    "# communityID = [index_to_Community_Mapper[x] for x in MaxSimilartyList]\n",
    "# communityWeight = np.array([communityScoreDict[y] for y in communityID])\n",
    "# # communityTextDict[index_to_Community_Mapper[MaxSimilartyList[0]]]\n",
    "# np.add(communitySimScore, communityWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection using Louvain Algorithm using Networkx (Undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a graph with its communities:\n",
    "# as Erdos-Renyi graphs don't have true community structure,\n",
    "# instead load the karate club graph\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "# compute the best partition\n",
    "partition = community_louvain.best_partition(G)\n",
    "print(\"partition-> \", partition)\n",
    "# draw the graph\n",
    "pos = nx.spring_layout(G)\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40, cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# nx.write_edgelist(G, \"test.edgelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection using Louvain Algorithm using Networkx (Directed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graph\n",
    "partition = community_louvain.best_partition(G)\n",
    "# modularity(partition, G)\n",
    "pos = nx.spring_layout(G)\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40, cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tesTensor = torch.FloatTensor([[[[1, 2, 3], [4, 5, 6], [7, 8, 9], [11, 12, 13]], [[14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25]], [[26, 27, 28], [29, 30, 31], [32, 33, 34], [35, 36, 37]]], [[[38, 39, 40], [41, 42, 43], [45, 46, 47], [48, 48, 50]], [[51, 52, 53], [54, 55, 56], [57, 58, 59], [60, 61, 62]], [[63, 64, 65], [66, 67, 68], [69, 70, 71], [72, 73, 74]]], [[[75, 76, 77], [78, 79, 80], [81, 82, 83], [84, 85, 86]], [[87, 88, 89], [90, 91, 92], [93, 94, 95], [96, 97, 98]], [[99, 100, 101], [102, 103, 104], [105, 106, 107], [108, 109, 110]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torch.mean(tesTensor[i][-2:, :,], dim=0).mean(dim=0) for i in range(len(tesTensor))]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(5/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustGCN",
   "language": "python",
   "name": "clustgcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
